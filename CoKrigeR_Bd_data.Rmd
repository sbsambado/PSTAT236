---
title: "CoKrigeR_Bd_data"
author: "Imani Russell"
date: "11/19/2020"
output:
  pdf_document: default
  html_document: default
---

#```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
#```

# load packages
```{r}
library(sp) 
library(gstat) 
```

# Read in the data, subset, and convert infection loads
```{r}
# read in the data
d <- read.csv("Dropbox/dissertation research/Summer_2019/Field_data_2019/field_data_11192020.csv", header=T, stringsAsFactors = F)

# remove frogs that we caught but didn't sample because they were too small
d <- d[d$Taxon != "too_small", ]

# remove frogs that are for bay area project
d <- d[d$bay_area_only != 1, ]

# remove weather data columns (I haven't had time to go through and add weather data)
df <- d [-c (28:35)]

# remove other columns that we're not using for spatial analysis
df <- df [-c (1, 6:8, 16, 26, 27:29)]

# check structure of df
str (df)

# ZE scores are showing as "character" class, so convert to numeric
df$ZE_1 <- as.numeric(df$ZE_1)
df$ZE_2 <- as.numeric(df$ZE_2)
df$ZE_3 <- as.numeric(df$ZE_3)

# convert mass and SVL to numeric
df$SVL_mm <- as.numeric(df$SVL_mm)
df$Mass_g <- as.numeric(df$Mass_g)
```

# calculate average ZE for each individual frog
```{r}
#df$avg_ze <- rowMeans (df [ ,c("ZE_1", "ZE_2", "ZE_3")], na.rm=TRUE)

# initialize empty vector for mean infection loads/individual
avg_ze <- numeric()

# loop to calculate mean
for (i in 1:nrow(df)){
  avg_ze[i] <- sum(df[i, c("ZE_1", "ZE_2", "ZE_3")])/3
}

# attach these vectors to the sub dataframe
df <- cbind (df, avg_ze)
```

# separate infected from uninfected --> log-transforming negative infections is creating problems down the line, and adding a small constant also results in highly skewed data even after log-transforming. So we will analyze positive infections only (negative infections are also really important, so I will need to figure out how to incorporate that, or if it's really just 2 separate analyses for the spatial analysis too)
```{r}
df <- df[!is.na(df$avg_ze), ]

# now I'm subsetting the dataframe we just made to include only positive infections
df <- df[df$avg_ze >= 1.0, ]
```

## Now going into exercises from Co-kriging with the gstat package (Rossiter, 2018)

# how do we make an interpolation grid? isn't kriging a method of interpolation?

# we want to map infection loads (target variable) - plot histograms of target variable (infection load) and log10 transformation
```{r}
require(lattice)

h_1 <- histogram (~ avg_ze, df, xlab = "Infection load", col = "thistle3")
h_2 <- histogram (~ log10(avg_ze), df, xlab = "log10(infection load)", col = "thistle3")

print (h_1, split = c(1, 1, 2, 1), more = T)
print (h_2, split = c(2, 1, 2, 1), more = F)

# remove these variables
rm (h_1, h_2)
```


# look at the proportions higher than each level
```{r}
# show the proportions higher than the various thresholds
ph <- function (level) {
  round (100 * sum (df$avg_ze > level) / length (df$avg_ze), 1)
}

p <- NULL; lvls <- c(100, 1000, 10000, 100000, 1000000)

for (l in lvls) p <- c(p, ph(l))

# display table of thresholds and proportions
(data.frame (cbind (level = lvls, percent.higher = p)))

rm (ph, l, lvls, p)
```

# choose (continuous) covariables based on my hypotheses
## SVL_mm: length of the frog from snout-to-vent (determines age class)
## mass_g: mass of the frog
## lat/long: coordinates where frog was captured <- how do we use this here?
## elevation
# choose covariables based on theory: organic matter content (OM) and zinc content (Zn)
```{r}
h1 <- histogram (~ SVL_mm, df, xlab = "SVL", col = "lightblue")
h2 <- histogram (~ Mass_g, df, xlab = "Mass", col = "red4")
h3 <- histogram (~ Elevation_m, df, xlab = "Elevation", col = "yellow")

h4 <- histogram (~ log10(SVL_mm), df, xlab = "log10(SVL)", col = "lightblue")
h5 <- histogram (~ log10(Mass_g), df, xlab = "log10(Mass)", col = "red4")
h6 <- histogram (~ log10(Elevation_m), df, xlab = "log10(Elevation)", col = "yellow")

print (h1, split = c(1, 1, 3, 2), more = T)
print (h2, split = c(2, 1, 3, 2), more = T)
print (h3, split = c(3, 1, 3, 2), more = T)
print (h4, split = c(1, 2, 3, 2), more = T)
print (h5, split = c(2, 2, 3, 2), more = T)
print (h6, split = c(3, 2, 3, 2), more = F)

rm (h1, h2, h3, h4, h5, h6)
```
It looks like elevation is slightly better without log-transforming, but the rest are better log10-transformed.

## Simulate under-sampling of target variable compared to the covariable

# Make subset of the observations of infection load (target variable) and covariables (SVL, mass, and elevation), using every third sample from the full dataset.
```{r}
df.inf <- df [seq (1, length (df$avg_ze), by = 3), 
                   c ("Latitude", "Longitude", "avg_ze", "SVL_mm", "Mass_g", "Elevation_m")]
str (df.inf)
```

# Add fields with the log10 transformed target and covariables to the dataframe of the subsample
```{r}
df.inf <- cbind (df.inf,
                   ltavgze = log10 (df.inf$avg_ze),
                   lt = log10 (df.inf$SVL_mm),
                   ltzn = log10 (df.inf$Mass_g))

str (df.inf)
```
The evaluation dataset is the rows of the dataset that were not used in the subset. We can use this to assess the performance of the interpolation. 

# Set up evaluation dataset: make dataframe of the infection load values at the extra points that were not included in the subsample and compare the descriptive statistics of the 3 sets (sample, extra, and full)

# *** adding a small constant to each avg infection load results in non-normality even after transforming, but non-transforming 0's results in -Inf. May need to 2 separate analyses here like with other stats and Bd -> 1 for positive infections and 1 for negative
```{r}
df.extra <- df [setdiff (rownames (df), rownames (df.inf)),
                      c ("Latitude", "Longitude", "avg_ze")]
df.extra <- cbind (df.extra, ltavgze = log10 (df.extra$avg_ze))

str (df.extra)

summary (log10 (df$avg_ze)); sd (log10 (df$avg_ze))

summary (df.inf$ltavgze); sd (df.inf$ltavgze)

summary (df.extra$ltavgze); sd (df.extra$ltavgze) # evaluation set
```
The subsample has very similar statistics to the full sample.
The range is narrower but standard deviation is a bit higher; median and mean are larger by chance. 
Evaluation has very similar statistics to the full set.

# Start the Spatial analysis!
# convert full and subset dataframes, and interpolation grid (need to create this), to explicitly-spatial, sp classes
```{r}
class(df)

coordinates (df) <- c ("Latitude", "Longitude")
coordinates (df.inf) <- c ("Latitude", "Longitude")
coordinates (df.extra) <- c ("Latitude", "Longitude")
#coordinates (meuse.grid) <- c ("Latitude", "Longitude")

class (df)
```
Now the objects (sample locations) are spatially-explicit - have a known bounding box, projection, and attributes

```{r}
summary (df.inf)
```

You can also recover the original dataframe if needed like this:
```{r}
str (as.data.frame (df))
```

# Display a postplot of the subsample superimposed on the full sample and compare their geographic distribution
```{r}
xyplot (Latitude ~ Longitude, as.data.frame (df), asp = "iso", 
        panel = function (x, ...) {
          panel.points (coordinates (df),
                        cex = 1.8 * (log10 (df$avg_ze) - 1.3),
                        pch = 1, col = "blue");
          panel.points (coordinates (df.inf), # subset
                        cex = 1.8 * (df.inf$ltavgze - 1.3),
                        pch = 20, col = "red");
          panel.grid (h = -1, v = -1, col = "darkgrey")
        })
```
